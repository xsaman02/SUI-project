import numpy as np
import json, math

class KNN():
	
	dataset_addr = "./datapoints.save"
	hashtable_addr = "./hashtable.save.json"
	dataset = np.asarray([])
	metrics = np.asarray([])
	metric_types = ()
	hashtable = {}
	
	def __init__(self, k : int) -> None:
		"""Init function\n
	    Parameters:
    	----------
		k (int): k sets number of nearest searching datapoints
		"""
		self.k = k


	def initialize(self, params : dict, n_data : int) -> None:
		"""		Function initialize new dataset generated by random.\n		
	    Parameters:
    	----------
		params (dict): key = name of feature, value = array (interval) for generating
		ndata (int): number of generated datapoints

		Important:
		----------
			datatypes of values in dictionary arrays will affect type of generated values!
		"""
		dataset = []
		for inter in params.values():
			t = type(inter[0])
			if t == int:
				fea_data = np.random.randint(inter[0], inter[1] + 1, n_data)
			else:
				""" Generetares vector of floats in given interval """
				""" 		x * interval + minimum_value		"""
				fea_data = np.random.rand(n_data) * (inter[1] - inter[0]) + inter[0]
			
			dataset.append(fea_data)

		self.dataset = np.asarray(dataset).transpose()
		return self.dataset

	def first_mapping(self, pos_datapoint, neg_datapoint, undecided=None) -> None:
		"""	This function is called after new inicialization.\n
		Starts mapping new datapoint to classes based on given datapoints.\n\n
		Given points will be excluded from dataset
		Also will hash datapoints for quicker search.

		Parameters:
		-----------
			pos-datapoint (np.array): positive datapoint
			neg-datapoint (np.array): negative datapoint
			undecided (np.array): help parameter for recursive evaluation
		"""

		if type(undecided) == type(None):
			self.set_new_data(pos_datapoint, True)
			self.set_new_data(neg_datapoint, False)
			dataset = self.dataset
		else:
			dataset = undecided
		
		undecided = np.array([])
		for dp in dataset:
			if str(dp) not in self.hashtable:
				r = self.evaluate(dp)
				if r == None:
					undecided = np.array([dp]) if undecided.size == 0 else np.append(undecided, [dp], axis=0)
				else:
					self.hashtable[str(dp)] = r > 0.5
		if undecided.size == 0:
			return
		else:
			self.first_mapping(None, None, undecided)

	def load_dataset(self) -> None:
		"""Function load dataset and hashtable from hardcoded addreses
		"""
		self.dataset = np.loadtxt(self.dataset_addr)
		with open(self.hashtable_addr, "r+") as fd:
			self.hashtable = json.load(fd)
		return self.dataset

	def evaluate(self, datapoint):
		"""		Function evaluate given datapoint (vector of features) on loaded dataset and returns index of recomended attacking (0-1)

	    Parameters:
    	----------
		datapoint (np.array): vector of features

		Returns:
		float: index of recomended attacking <0-1>
		None in case of undecided

		Important:
		----------
		If dataset and metrics is not set, will raise exception
		"""
		assert(self.dataset.size != 0)
		
		datapoints = self.__get_k_neighbors(datapoint)
		classes = np.array([self.get_class_of_datapoint(dp[0]) for dp in datapoints])
		pos = np.count_nonzero(classes == True)
		neg = np.count_nonzero(classes == False)
		if pos + neg == 0:
			return None
		return pos / (pos + neg)




	def set_new_data(self, data, y : bool) -> None:
		"""	Function adds new data (vector of features) to the dataset and marked it by class y (0, 1)

		Args:
			data (numpy.array): vector of features
			y (bool): class
		"""
		assert(self.dataset.size != 0)
		self.dataset = np.append(self.dataset, [data], axis=0)
		self.hashtable[str(self.__round_metric(data))] = y


	def set_metrics(self, metrics, types) -> None:
		"""	Function takes metrics (array of intervals) on which will start searching nearest neighbors\n
		
	    Parameters:
    	----------
		metrics (np.array): vector of deviations
		types (list):	list of types of metrics

		Example:
    	----------
		1D data:
			metrics: (0.5)
			data: (1.0)
			searching pool for nearest neighbors: (0.5 - 1.5)\n
		
		Metrics needs to have same dimension as input data.\n
		Info:
		----
		Metrics set up is good for cutting searching and computing time\n
		and is mandatory to set up before evaluating.\n
		Integer values will be always rouded up.
		"""
		self.metrics = metrics
		self.metric_types = types

	def save_dataset(self) -> None:
		"""Saves loaded dataset to hard-coded addr
		"""
		np.savetxt(self.dataset_addr, self.dataset)
		with open(self.hashtable_addr, "w") as fd:
			json.dump(self.hashtable, fd)

	def __compute_euclid_dist(self, a, b) -> float:
		"""		Function takes vectors of two datapoints and compute euclid distance between them\n
		parameters: a, b both needs to be numpy arrays\n

		Parameters
		----------
		a (np.array): datapoint1
		b (np.array): datapoint2

		Returns:
		float: euclid distance
		"""
		return np.linalg.norm(b-a)

	def __get_k_neighbors(self, datapoint, metrics=None, selected_datapoints=np.array([]), last_metric=None, recur_count=0) -> np.ndarray:
		"""Get k nearest neighbors from given datapoint

		Parameters:
		-----------
			datapoint (np.array): Given datapoint
			metrics (np.array, optional): Custom metrics for setting up searching pool. Defaults to None.
			selected-datapoints (np.array, optional): Selected points for no need searching in whole dataset. Defaults to empty np.array.
			last-metric (np.array): Help parametr for computing next step of metric
		Returns:
		--------
			np.ndarray: Returns K nearest neighbors
		"""
		

		assert(type(self.metrics) != type(None))

		if type(metrics) == type(None):
			metrics = self.metrics


		v_b, v_s = None, None
		if selected_datapoints.size != 0:
			v_b = np.all(selected_datapoints >= self.__round_metric(datapoint - metrics), axis=1)
			v_s = np.all(selected_datapoints <= self.__round_metric(datapoint + metrics), axis=1)
			
			i = np.argwhere(v_b==True)
			v_b = selected_datapoints[i]
			i = np.argwhere(v_s==True)
			v_s = selected_datapoints[i]
		else:
			v_b = np.all(self.dataset >= (datapoint - metrics), axis=1)
			v_s = np.all(self.dataset <= (datapoint + metrics), axis=1)
		
			i = np.argwhere(v_b==True)
			v_b = self.dataset[i]
			i = np.argwhere(v_s==True)
			v_s = self.dataset[i]


		datapoints = np.array([v for v in v_b if v in v_s])
		if datapoint.tolist() in datapoints.tolist():
			datapoints.remove(datapoint)

		if self.k <= len(datapoints) or recur_count > 5:
			if len(datapoints) <= 2*self.k or recur_count > 5:
				if len(datapoints) == self.k:
					return datapoints
				distances = []
				for d in datapoints:
					distances.append(self.__compute_euclid_dist(datapoint, d))
				k_min = np.argsort(distances)[0:self.k]
				return np.array(datapoints)[k_min]
			else:
				if type(last_metric) == type(None):
					difference = metrics / 2
				else:	
					difference = metrics - abs(metrics - last_metric) * 0.5
				
				# print("too much ", len(datapoints), difference)
				return self.__get_k_neighbors(datapoint, difference, np.asarray(selected_datapoints), last_metric=metrics, recur_count=recur_count+1)
		else:
			if type(last_metric) == type(None):
				difference = 2*metrics
				# print("too little ", len(datapoints), difference)
				return self.__get_k_neighbors(datapoint, difference, np.asarray(selected_datapoints), last_metric=None, recur_count=recur_count+1)

			else: 
				difference = metrics + abs(metrics - last_metric) * 0.5
				# print("too little ", len(datapoints), difference)
				return self.__get_k_neighbors(datapoint, difference, np.asarray(selected_datapoints), last_metric=metrics, recur_count=recur_count+1)
	


	def get_class_of_datapoint(self, dp) -> bool:
		"""	Function returns class of given datapoint\n
			If class is not known, returns None
		"""
		h = str(self.__round_metric(dp))
		if h in self.hashtable:
			return self.hashtable[h]
		return None

	def __round_metric(self, metric):
		for i, type in enumerate(self.metric_types):
			if type == int:
				metric[i] = math.ceil(metric[i])
		return metric



import time
import matplotlib.pyplot as plt
if __name__ == "__main__":
	s = time.perf_counter()
	knn = KNN(7)
	
	# d = {"enemy teritories" : [0, 10], "chance to capture" : [0.0, 1.0], "chance to sustain" : [0.0, 1.0], "enemy teritories2" : [0, 10], "chance to capture2" : [0.0, 1.0], "chance to sustain2" : [0.0, 1.0], "chance to sustain3" : [0.0, 1.0]}
	d = {"chance to capture" : [0.0, 10.0], "chance to sustain" : [0.0, 10.0]}
	
	# performance = []
	# knn.set_metrics(np.array([1, 0.1, 0.1, 1, 0.1, 0.1, 1]), (int, float, float, int, float, float, float))
	# knn.load_dataset()
	knn.set_metrics(np.array([1.0, 1.0]), (float, float))
	knn.initialize(d, 200)
	pos = np.array([9.33, 0.13])
	neg = np.array([0.94, 0.88])
	knn.first_mapping(pos, neg)
	# knn.save_dataset()

	plt.figure()
	dps = knn.dataset
	cs = [knn.get_class_of_datapoint(dp) for dp in dps]

	pos_dps, neg_dps = [], []
	for d, c in zip(dps, cs):
		if c:
			pos_dps.append(d)
		else:
			neg_dps.append(d)

	pos_dps = np.asarray(pos_dps).transpose()
	neg_dps = np.asarray(neg_dps).transpose()

	plt.scatter(pos_dps[0], pos_dps[1], color="green")
	plt.scatter(neg_dps[0], neg_dps[1], color="orange")
	plt.show()
	
